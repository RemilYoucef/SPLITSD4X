{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the libraries \" \n",
    "\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the scripts of SD for Explaining and the supplementary scripts for neighbors generation\"\n",
    "\n",
    "absFilePath = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "newPath = os.path.join(absFilePath, 'SplitSD4X\\\\')\n",
    "sys.path.append(newPath)\n",
    "\n",
    "newPath_supp = os.path.join(newPath, 'supplementary')\n",
    "sys.path.append(newPath_supp)\n",
    "\n",
    "from fill_missing_values import *\n",
    "from missing_values_table import *\n",
    "from subgroups_discovery import *\n",
    "\n",
    "from neighbors_generation import *\n",
    "from neighbors_generation_2 import *\n",
    "from neighbors_generation_3 import *\n",
    "from neighbors_generation_4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Loading the dataset \"\n",
    "datasets_path = os.path.join(absFilePath, 'Datasets\\\\')\n",
    "url = datasets_path + 'data_adult_income.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "\" Handling some data \"\n",
    "\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df = df.drop(['capital-gain','capital-loss','native-country'],axis =1)\n",
    "df['age'] = df['age'].astype(float)\n",
    "df['fnlwgt'] = df['fnlwgt'].astype(float)\n",
    "df['educational-num'] = df['educational-num'].astype(float)\n",
    "df['hours-per-week'] = df['hours-per-week'].astype(float)\n",
    "df['income'] = df['income'].map({ \"<=50K\": 1, \">50K\": 2 })\n",
    "\n",
    "data_df = df.drop(columns=['income'])\n",
    "target_df = df['income']\n",
    "\n",
    "categorical_feature_mask = (data_df.dtypes == object)\n",
    "categorical_cols_names = data_df.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols_names = data_df.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "data_df = pd.concat([data_df[numerical_cols_names], data_df[categorical_cols_names]],axis = 1)\n",
    "data_target_df = pd.concat([data_df, target_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" generate the Test SET \"\n",
    "a = False \n",
    "b = False\n",
    "c = False\n",
    "d = False \n",
    "e = False\n",
    "f = False\n",
    "g = False\n",
    "\n",
    "while (not a or not b or not c or not d or not e or not f or not g ) : \n",
    "\n",
    "    nb_test_instances = 1000 \n",
    "    test_df = data_target_df.sample(n=nb_test_instances)\n",
    "    data_test_df = test_df.drop(columns=['income'])\n",
    "    target_test_df = test_df['income']\n",
    "\n",
    "\n",
    "    a = (data_df['workclass'].unique().size == data_test_df['workclass'].unique().size)\n",
    "    b = (data_df['education'].unique().size == data_test_df['education'].unique().size)\n",
    "    c = (data_df['marital-status'].unique().size == data_test_df['marital-status'].unique().size)\n",
    "    d = (data_df['occupation'].unique().size == data_test_df['occupation'].unique().size)\n",
    "    e = (data_df['relationship'].unique().size == data_test_df['relationship'].unique().size)\n",
    "    f = (data_df['race'].unique().size == data_test_df['race'].unique().size)\n",
    "    g = (data_df['gender'].unique().size == data_test_df['gender'].unique().size)\n",
    "\n",
    "\" generate the Training SET \"\n",
    "train_df = pd.concat([data_target_df,test_df]).drop_duplicates(keep=False)\n",
    "data_train_df = train_df.drop(columns=['income'])\n",
    "target_train_df = train_df['income']\n",
    "\n",
    "\" Decode Categorical Features \"\n",
    "workclass_mapper = {\n",
    "    0 : 'Private',\n",
    "    1 : 'Local-gov',\n",
    "    2 : 'Self-emp-not-inc',\n",
    "    3 : 'Federal-gov',\n",
    "    4 : 'State-gov',\n",
    "    5 : 'Self-emp-inc',\n",
    "    6 : 'Without-pay'\n",
    "}\n",
    "workclass_mapper_inv = dict(map(reversed, workclass_mapper.items()))\n",
    "\n",
    "'------------------------------------------------------------------------'\n",
    "\n",
    "education_mapper = {\n",
    "    0 : '11th', 1 : 'HS-grad', 2 : 'Assoc-acdm', 3 : 'Some-college',\n",
    "    4 : '10th', 5 : 'Prof-school', 6 : '7th-8th', 7 : 'Bachelors', \n",
    "    8 : 'Masters' , 9 : '5th-6th', 10 : 'Assoc-voc', 11 : '9th' ,\n",
    "    12 : 'Doctorate', 13 : '12th', 14 : '1st-4th', 15 : 'Preschool'\n",
    "}\n",
    "education_mapper_inv = dict(map(reversed, education_mapper.items()))\n",
    "\n",
    "'------------------------------------------------------------------------'\n",
    "\n",
    "maritalStatus_mapper = {\n",
    "    0 : 'Never-married', 1 : 'Married-civ-spouse', 2 : 'Widowed', \n",
    "    3 : 'Separated',4 : 'Divorced', 5 : 'Married-spouse-absent', \n",
    "    6 : 'Married-AF-spouse'\n",
    "}\n",
    "maritalStatus_mapper_inv = dict(map(reversed, maritalStatus_mapper.items()))\n",
    "\n",
    "'------------------------------------------------------------------------'\n",
    "\n",
    "occupation_mapper = {\n",
    "    0 : 'Machine-op-inspct', 1 : 'Farming-fishing', 2 : 'Protective-serv', 3 : 'Other-service',\n",
    "    4 : 'Prof-specialty', 5 : 'Craft-repair', 6 : 'Adm-clerical', 7 : 'Exec-managerial', \n",
    "    8 : 'Tech-support' , 9 : 'Sales', 10 : 'Priv-house-serv', 11 : 'Transport-moving' ,\n",
    "    12 : 'Handlers-cleaners', 13 : 'Armed-Forces'\n",
    "}\n",
    "occupation_mapper_inv = dict(map(reversed, occupation_mapper.items()))\n",
    "\n",
    "'------------------------------------------------------------------------'\n",
    "\n",
    "relationship_mapper = {\n",
    "    0 : 'Own-child', 1 : 'Husband', 2 : 'Not-in-family', \n",
    "    3 : 'Unmarried', 4 : 'Wife', 5 : 'Other-relative'\n",
    "}\n",
    "relationship_mapper_inv = dict(map(reversed, relationship_mapper.items()))\n",
    "\n",
    "'------------------------------------------------------------------------'\n",
    "\n",
    "race_mapper = {\n",
    "    0 : 'Black', 1 : 'White', 2 : 'Other', \n",
    "    3 : 'Amer-Indian-Eskimo', 4 : 'Asian-Pac-Islander'\n",
    "}\n",
    "race_mapper_inv = dict(map(reversed, race_mapper.items()))\n",
    "\n",
    "'------------------------------------------------------------------------'\n",
    "gender_mapper = {\n",
    "    0 : 'Male', 1 : 'Female'\n",
    "}\n",
    "gender_mapper_inv = dict(map(reversed, gender_mapper.items()))\n",
    "\n",
    "\n",
    "data_test_df_copy = data_test_df.copy()\n",
    "\n",
    "data_test_df_copy['workclass'] = data_test_df_copy['workclass'].replace(workclass_mapper_inv)\n",
    "data_test_df_copy['education'] = data_test_df_copy['education'].replace(education_mapper_inv)\n",
    "data_test_df_copy['marital-status'] = data_test_df_copy['marital-status'].replace(maritalStatus_mapper_inv)\n",
    "data_test_df_copy['occupation'] = data_test_df_copy['occupation'].replace(occupation_mapper_inv)\n",
    "data_test_df_copy['relationship'] = data_test_df_copy['relationship'].replace(relationship_mapper_inv)\n",
    "data_test_df_copy['race'] = data_test_df_copy['race'].replace(race_mapper_inv)\n",
    "data_test_df_copy['gender'] = data_test_df_copy['gender'].replace(gender_mapper_inv)\n",
    "\n",
    "data_test_copy = data_test_df_copy.values\n",
    "numerical_cols = np.arange(0,len(numerical_cols_names)) \n",
    "categorical_cols = np.arange(len(numerical_cols_names),data_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 1*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neighbors = 50\n",
    "list_neigh = generate_all_neighbors(data_test_copy,numerical_cols,categorical_cols,nb_neighbors)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors = list_neigh[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors = np.concatenate((all_neighbors, list_neigh[i]), axis=0)\n",
    "    \n",
    "\" One hot encoding \"\n",
    "\n",
    "df_neigh = pd.DataFrame(data = all_neighbors,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh[categorical_cols_names] = df_neigh[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "\n",
    "df_neigh['workclass'] = df_neigh['workclass'].replace(workclass_mapper)\n",
    "df_neigh['education'] = df_neigh['education'].replace(education_mapper)\n",
    "df_neigh['marital-status'] = df_neigh['marital-status'].replace(maritalStatus_mapper)\n",
    "df_neigh['occupation'] = df_neigh['occupation'].replace(occupation_mapper)\n",
    "df_neigh['relationship'] = df_neigh['relationship'].replace(relationship_mapper)\n",
    "df_neigh['race'] = df_neigh['race'].replace(race_mapper)\n",
    "df_neigh['gender'] = df_neigh['gender'].replace(gender_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh = pd.get_dummies(df_neigh, prefix_sep='_', drop_first=True)\n",
    "\n",
    "\" Scale the neighbors data \"\n",
    "data_neigh = df_neigh.values\n",
    "\n",
    "scaler_neigh = StandardScaler()\n",
    "data_neigh_s = scaler_neigh.fit_transform(data_neigh)\n",
    "\n",
    "\" Store the neighbors in a list \"\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh.append(data_neigh_s[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 2*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_nb_categ = []\n",
    "j = 0 \n",
    "for name in categorical_cols_names :\n",
    "    mat_nb_categ.append(np.size(data_df[name].unique()))\n",
    "\n",
    "nb_neighbors = 50\n",
    "list_neigh_2 = generate_all_neighbors_2(data_test_copy,numerical_cols,categorical_cols,mat_nb_categ,nb_neighbors)\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors_2 = list_neigh_2[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors_2 = np.concatenate((all_neighbors_2, list_neigh_2[i]), axis=0)\n",
    "    \n",
    "df_neigh_2 = pd.DataFrame(data = all_neighbors_2,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh_2[categorical_cols_names] = df_neigh_2[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "\n",
    "df_neigh_2['workclass'] = df_neigh_2['workclass'].replace(workclass_mapper)\n",
    "df_neigh_2['education'] = df_neigh_2['education'].replace(education_mapper)\n",
    "df_neigh_2['marital-status'] = df_neigh_2['marital-status'].replace(maritalStatus_mapper)\n",
    "df_neigh_2['occupation'] = df_neigh_2['occupation'].replace(occupation_mapper)\n",
    "df_neigh_2['relationship'] = df_neigh_2['relationship'].replace(relationship_mapper)\n",
    "df_neigh_2['race'] = df_neigh_2['race'].replace(race_mapper)\n",
    "df_neigh_2['gender'] = df_neigh_2['gender'].replace(gender_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh_2 = pd.get_dummies(df_neigh_2, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_neigh_2 = df_neigh_2.values\n",
    "\n",
    "scaler_neigh = StandardScaler()\n",
    "data_neigh_2s = scaler_neigh.fit_transform(data_neigh_2)\n",
    "\n",
    "\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh_2 = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh_2.append(data_neigh_2s[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 3*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neigh_3 = generate_all_neighbors_3(data_test_copy,numerical_cols,categorical_cols,mat_nb_categ,nb_neighbors)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors_3 = list_neigh_3[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors_3 = np.concatenate((all_neighbors_3, list_neigh_3[i]), axis=0)\n",
    "    \n",
    "df_neigh_3 = pd.DataFrame(data = all_neighbors_3,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh_3[categorical_cols_names] = df_neigh_3[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "\n",
    "df_neigh_3['workclass'] = df_neigh_3['workclass'].replace(workclass_mapper)\n",
    "df_neigh_3['education'] = df_neigh_3['education'].replace(education_mapper)\n",
    "df_neigh_3['marital-status'] = df_neigh_3['marital-status'].replace(maritalStatus_mapper)\n",
    "df_neigh_3['occupation'] = df_neigh_3['occupation'].replace(occupation_mapper)\n",
    "df_neigh_3['relationship'] = df_neigh_3['relationship'].replace(relationship_mapper)\n",
    "df_neigh_3['race'] = df_neigh_3['race'].replace(race_mapper)\n",
    "df_neigh_3['gender'] = df_neigh_3['gender'].replace(gender_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh_3 = pd.get_dummies(df_neigh_3, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_neigh_3 = df_neigh_3.values\n",
    "\n",
    "scaler_neigh = StandardScaler()\n",
    "data_neigh_3s = scaler_neigh.fit_transform(data_neigh_3)\n",
    "\n",
    "\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh_3 = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh_3.append(data_neigh_3s[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 4*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = []\n",
    "\n",
    "list_neigh_4 = generate_all_neighbors_4(data_test_copy,numerical_cols,categorical_cols,mat_nb_categ,nb_neighbors,special)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors_4 = list_neigh_4[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors_4 = np.concatenate((all_neighbors_4, list_neigh_4[i]), axis=0)\n",
    "    \n",
    "df_neigh_4 = pd.DataFrame(data = all_neighbors_4,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh_4[categorical_cols_names] = df_neigh_4[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "\n",
    "df_neigh_4['workclass'] = df_neigh_4['workclass'].replace(workclass_mapper)\n",
    "df_neigh_4['education'] = df_neigh_4['education'].replace(education_mapper)\n",
    "df_neigh_4['marital-status'] = df_neigh_4['marital-status'].replace(maritalStatus_mapper)\n",
    "df_neigh_4['occupation'] = df_neigh_4['occupation'].replace(occupation_mapper)\n",
    "df_neigh_4['relationship'] = df_neigh_4['relationship'].replace(relationship_mapper)\n",
    "df_neigh_4['race'] = df_neigh_4['race'].replace(race_mapper)\n",
    "df_neigh_4['gender'] = df_neigh_4['gender'].replace(gender_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh_4 = pd.get_dummies(df_neigh_4, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_neigh_4 = df_neigh_4.values\n",
    "\n",
    "scaler_neigh = StandardScaler()\n",
    "data_neigh_4s = scaler_neigh.fit_transform(data_neigh_4)\n",
    "\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh_4 = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh_4.append(data_neigh_4s[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One hot encoding for the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df = pd.get_dummies(data_train_df, prefix_sep='_', drop_first=True)\n",
    "data_train = data_train_df.values\n",
    "target_train = target_train_df.values\n",
    "\n",
    "data_test_df = pd.get_dummies(data_test_df, prefix_sep='_', drop_first=True)\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values\n",
    "\n",
    "\" Scale the training and the test sets data\"\n",
    "scaler_train = StandardScaler()\n",
    "data_train_s = scaler_train.fit_transform(data_train)\n",
    "\n",
    "scaler_test = StandardScaler()\n",
    "data_test_s = scaler_test.fit_transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Sklearn MLP Classifier \"\n",
    "\n",
    "mlp = MLPClassifier(activation='logistic',hidden_layer_sizes=(50,50), max_iter=5000,\n",
    "                    solver='sgd', random_state=1,alpha=0.1,\n",
    "                    learning_rate_init=.1)\n",
    "model_nt = mlp.fit(data_train_s, target_train)\n",
    "target_pred_mlp = model_nt.predict(data_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of Split Based Selection Form Algorithm : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(numerical_cols)\n",
    "nb_models = 100\n",
    "(L_Subgroups_1,P_1) = SplitBasedSelectionForm (data_test_s, target_test, nb_models, model_nt, list_neigh,split_point,2)\n",
    "(L_Subgroups_2,P_2) = SplitBasedSelectionForm (data_test_s, target_test, nb_models, model_nt, list_neigh_2,split_point,2)\n",
    "(L_Subgroups_3,P_3) = SplitBasedSelectionForm (data_test_s, target_test, nb_models, model_nt, list_neigh_3,split_point,2)\n",
    "(L_Subgroups_4,P_4) = SplitBasedSelectionForm (data_test_s, target_test, nb_models, model_nt, list_neigh_4,split_point,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Define the functions to save and load data \"\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE DATA'\n",
    "\n",
    "path = './saved_data/'\n",
    "save_obj(data_train_s, path + 'data_train')\n",
    "save_obj(target_train, path + 'target_train')\n",
    "save_obj(data_test_s,  path + 'data_test')\n",
    "save_obj(target_test,  path + 'target_test')\n",
    "save_obj(list_neigh ,  path + 'list_neighbors_1')\n",
    "save_obj(list_neigh_2, path + 'list_neighbors_2')\n",
    "save_obj(list_neigh_3, path + 'list_neighbors_3')\n",
    "save_obj(list_neigh_4, path + 'list_neighbors_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE LIST OF THE SUBGROUPS'\n",
    "save_obj(L_Subgroups_1, path + 'list_subgroups_1')\n",
    "save_obj(L_Subgroups_2, path + 'list_subgroups_2')\n",
    "save_obj(L_Subgroups_3, path + 'list_subgroups_3')\n",
    "save_obj(L_Subgroups_4, path + 'list_subgroups_4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
