{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the libraries \" \n",
    "\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the scripts of SD for Explaining and the supplementary scripts for neighbors generation\"\n",
    "\n",
    "absFilePath = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "newPath = os.path.join(absFilePath, 'SplitSD4X\\\\')\n",
    "sys.path.append(newPath)\n",
    "\n",
    "newPath_supp = os.path.join(newPath, 'supplementary')\n",
    "sys.path.append(newPath_supp)\n",
    "\n",
    "from fill_missing_values import *\n",
    "from missing_values_table import *\n",
    "from subgroups_discovery import *\n",
    "\n",
    "from neighbors_generation import *\n",
    "from discretization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Loading the dataset \"\n",
    "datasets_path = os.path.join(absFilePath, 'Datasets\\\\')\n",
    "url = datasets_path + 'data_gpu_performances.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.sample(n=10000)\n",
    "\n",
    "mwg_nwg_mapper = {16 : 1, 32 : 2, 64 : 3, 128 : 4}\n",
    "df['MWG'] = df['MWG'].replace(mwg_nwg_mapper)\n",
    "df['NWG'] = df['NWG'].replace(mwg_nwg_mapper)\n",
    "\n",
    "kwg_mapper = {16 : 1, 32 : 2}\n",
    "df['KWG'] = df['KWG'].replace(kwg_mapper)\n",
    "\n",
    "mdi_ndi_mapper =  {8 : 1, 16 : 2, 32 : 3}\n",
    "df['MDIMC'] = df['MDIMC'].replace(mdi_ndi_mapper)\n",
    "df['NDIMC'] = df['NDIMC'].replace(mdi_ndi_mapper)\n",
    "df['MDIMA'] = df['MDIMA'].replace(mdi_ndi_mapper)\n",
    "df['NDIMB'] = df['NDIMB'].replace(mdi_ndi_mapper)\n",
    "\n",
    "kwi_mapper = {2 : 1, 8 : 2}\n",
    "df['KWI'] = df['KWI'].replace(kwi_mapper)\n",
    "\n",
    "vwm_vwn_mapper =  {1 : 1, 2 : 2, 4 : 3, 8 : 4}\n",
    "df['VWM'] = df['VWM'].replace(mdi_ndi_mapper)\n",
    "df['VWM'] = df['VWN'].replace(mdi_ndi_mapper)\n",
    "\n",
    "df['Run'] = (df['Run1 (ms)'] + df['Run2 (ms)'] + df['Run3 (ms)'] + df['Run4 (ms)']) / 4\n",
    "df = df.drop(['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'],axis =1)\n",
    "\n",
    "str_mapper = {0 : 'disable', 1 : 'enable'}\n",
    "str_mapper_inv = dict(map(reversed, str_mapper.items()))\n",
    "df['STRM'] = df['STRM'].replace(str_mapper)\n",
    "df['STRN'] = df['STRN'].replace(str_mapper)\n",
    "\n",
    "s_mapper = {0 : 'N', 1 : 'T'}\n",
    "s_mapper_inv = dict(map(reversed, s_mapper.items()))\n",
    "df['SA'] = df['SA'].replace(s_mapper)\n",
    "df['SB'] = df['SB'].replace(s_mapper)\n",
    "\n",
    "\" separate the data and the target \"\n",
    "data_df = df.drop(columns=['Run'])\n",
    "target_df = df['Run']\n",
    "\n",
    "categorical_feature_mask = (data_df.dtypes == object)\n",
    "categorical_cols_names = data_df.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols_names = data_df.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "data_df = pd.concat([data_df[numerical_cols_names], data_df[categorical_cols_names]],axis = 1)\n",
    "\n",
    "\n",
    "\" Encoding categorical features\"\n",
    "\n",
    "data_df['STRM'] = data_df['STRM'].replace(str_mapper_inv)\n",
    "data_df['STRN'] = data_df['STRN'].replace(str_mapper_inv)\n",
    "data_df['SA'] = data_df['SA'].replace(s_mapper_inv)\n",
    "data_df['SB'] = data_df['SB'].replace(s_mapper_inv)\n",
    "\n",
    "data_target_df = pd.concat([data_df, target_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" generate the Test SET \"\n",
    "nb_test_instances = 1000 \n",
    "test_df = data_target_df.sample(n=nb_test_instances)\n",
    "data_test_df = test_df.drop(columns=['Run'])\n",
    "target_test_df = test_df['Run']\n",
    "\n",
    "\" generate the Training SET \"\n",
    "train_df = pd.concat([data_target_df,test_df]).drop_duplicates(keep=False)\n",
    "data_train_df = train_df.drop(columns=['Run'])\n",
    "target_train_df = train_df['Run']\n",
    "\n",
    "\" Extract values of the test set to generate the neighbors\"\n",
    "\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values\n",
    "\n",
    "numerical_cols = np.arange(0,len(numerical_cols_names)) \n",
    "categorical_cols = np.arange(len(numerical_cols_names),data_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neighbors = 20\n",
    "list_neigh = generate_all_neighbors(data_test,numerical_cols,categorical_cols,nb_neighbors)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test,0)\n",
    "all_neighbors = list_neigh[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors = np.concatenate((all_neighbors, list_neigh[i]), axis=0)\n",
    "    \n",
    "\" One hot encoding \"\n",
    "\n",
    "df_neigh = pd.DataFrame(data = all_neighbors,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh[categorical_cols_names] = df_neigh[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "df_neigh['STRM'] = df_neigh['STRM'].replace(str_mapper)\n",
    "df_neigh['STRN'] = df_neigh['STRN'].replace(str_mapper)\n",
    "df_neigh['SA'] = df_neigh['SA'].replace(s_mapper)\n",
    "df_neigh['SB'] = df_neigh['SB'].replace(s_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh = pd.get_dummies(df_neigh, prefix_sep='_', drop_first=True)\n",
    "\n",
    "\" Store the neighbors in a list\"\n",
    "\n",
    "data_neigh = df_neigh.values\n",
    "n = np.size(data_test,0)\n",
    "list_neigh = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh.append(data_neigh[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One hot encoding for the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df['STRM'] = data_train_df['STRM'].replace(str_mapper)\n",
    "data_train_df['STRN'] = data_train_df['STRN'].replace(str_mapper)\n",
    "data_train_df['SA'] = data_train_df['SA'].replace(s_mapper)\n",
    "data_train_df['SB'] = data_train_df['SB'].replace(s_mapper)\n",
    "\n",
    "data_train_df = pd.get_dummies(data_train_df, prefix_sep='_', drop_first=True)\n",
    "data_train = data_train_df.values\n",
    "target_train = target_train_df.values\n",
    "\n",
    "data_test_df['STRM'] = data_test_df['STRM'].replace(str_mapper)\n",
    "data_test_df['STRN'] = data_test_df['STRN'].replace(str_mapper)\n",
    "data_test_df['SA'] = data_test_df['SA'].replace(s_mapper)\n",
    "data_test_df['SB'] = data_test_df['SB'].replace(s_mapper)\n",
    "\n",
    "data_test_df = pd.get_dummies(data_test_df, prefix_sep='_', drop_first=True)\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Sklearn MLP regressor \"\n",
    "\n",
    "mlp = make_pipeline(StandardScaler(),\n",
    "                    MLPRegressor(hidden_layer_sizes=(50, 50),\n",
    "                                 tol=1e-2, \n",
    "                                 max_iter=1000, \n",
    "                                 random_state=0))\n",
    "model_nt = mlp.fit(data_train, target_train)\n",
    "target_pred_nt = model_nt.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of Split Based Selection Form Algorithm : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization : Equal Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(numerical_cols)\n",
    "nb_models = 100\n",
    "L_Subgroups_freq = []\n",
    "\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,4)[0])\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,5)[0])\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,6)[0])\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,7)[0])\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,8)[0])\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,9)[0])\n",
    "L_Subgroups_freq.append(SplitBasedSelectionForm_freq (data_test, target_test, nb_models, model_nt, list_neigh,split_point,10)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization : Equal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Subgroups_width = []\n",
    "\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,4)[0])\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,5)[0])\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,6)[0])\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,7)[0])\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,8)[0])\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,9)[0])\n",
    "L_Subgroups_width.append(SplitBasedSelectionForm_width (data_test, target_test, nb_models, model_nt, list_neigh,split_point,10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Define the functions to save and load data \"\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE DATA'\n",
    "\n",
    "path = './saved_data/'\n",
    "save_obj(data_train, path + 'data_train_d')\n",
    "save_obj(target_train, path + 'target_train_d')\n",
    "save_obj(data_test, path  + 'data_test_d')\n",
    "save_obj(target_test, path + 'target_test_d')\n",
    "save_obj(list_neigh, path   + 'list_neighbors_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE LIST OF THE SUBGROUPS'\n",
    "save_obj(L_Subgroups_freq, path + 'l_list_subgroups_freq')\n",
    "save_obj(L_Subgroups_width, path + 'l_list_subgroups_width')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
