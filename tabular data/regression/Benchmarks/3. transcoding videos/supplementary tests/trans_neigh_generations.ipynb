{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the libraries \" \n",
    "\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the scripts of SD for Explaining and the supplementary scripts for neighbors generation\"\n",
    "\n",
    "absFilePath = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "newPath = os.path.join(absFilePath, 'SplitSD4X\\\\')\n",
    "sys.path.append(newPath)\n",
    "\n",
    "newPath_supp = os.path.join(newPath, 'supplementary')\n",
    "sys.path.append(newPath_supp)\n",
    "\n",
    "from fill_missing_values import *\n",
    "from missing_values_table import *\n",
    "from subgroups_discovery import *\n",
    "\n",
    "from neighbors_generation import *\n",
    "from neighbors_generation_2 import *\n",
    "from neighbors_generation_3 import *\n",
    "from neighbors_generation_4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Loading the dataset \"\n",
    "datasets_path = os.path.join(absFilePath, 'Datasets\\\\')\n",
    "url = datasets_path + 'data_transcoding_mesurment.tsv'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "\" Handling some data \"\n",
    "df = df.drop(columns=['id'])\n",
    "df = df.rename(columns={'i': 'i_frames', 'p': 'p_frames' , 'b':'b_frames'})\n",
    "\n",
    "data_df = df.drop(columns=['utime'])\n",
    "target_df = df['utime']\n",
    "\n",
    "categorical_feature_mask = (data_df.dtypes == object)\n",
    "categorical_cols_names = data_df.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols_names = data_df.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "data_df = pd.concat([data_df[numerical_cols_names].astype(float), data_df[categorical_cols_names]],axis = 1)\n",
    "data_target_df = pd.concat([data_df, target_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" generate the Test SET \"\n",
    "nb_test_instances = 1000 \n",
    "test_df = data_target_df.sample(n=nb_test_instances)\n",
    "data_test_df = test_df.drop(columns=['utime'])\n",
    "target_test_df = test_df['utime']\n",
    "\n",
    "\" generate the Training SET \"\n",
    "train_df = pd.concat([data_target_df,test_df]).drop_duplicates(keep=False)\n",
    "data_train_df = train_df.drop(columns=['utime'])\n",
    "target_train_df = train_df['utime']\n",
    "\n",
    "\" Extract values of the test set to generate the neighbors\"\n",
    "\n",
    "\" Decode Categorical Features \"\n",
    "\n",
    "format_mapper = {\n",
    "    0 : 'vp8', 1 : 'h264', 2 : 'mpeg4', 3 : 'flv'\n",
    "}\n",
    "format_mapper_inv = dict(map(reversed, format_mapper.items()))\n",
    "\n",
    "data_test_df_copy = data_test_df.copy()\n",
    "data_test_df_copy['codec']   = data_test_df_copy['codec'].replace(format_mapper_inv)\n",
    "data_test_df_copy['o_codec'] = data_test_df_copy['o_codec'].replace(format_mapper_inv)\n",
    "\n",
    "data_test_copy = data_test_df_copy.values\n",
    "numerical_cols = np.arange(0,len(numerical_cols_names)) \n",
    "categorical_cols = np.arange(len(numerical_cols_names),data_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 1*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youcef\\Desktop\\PFE\\SplitSD4X\\Regression\\SplitSD4X\\neighbors_generation.py:15: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  return np.random.multivariate_normal(inst_num,covn,n)\n"
     ]
    }
   ],
   "source": [
    "# generate neighbors : \n",
    "nb_neighbors = 50\n",
    "list_neigh = generate_all_neighbors(data_test_copy,numerical_cols,categorical_cols,nb_neighbors)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors = list_neigh[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors = np.concatenate((all_neighbors, list_neigh[i]), axis=0)\n",
    "    \n",
    "\" One hot encoding \"\n",
    "\n",
    "df_neigh = pd.DataFrame(data = all_neighbors,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh[categorical_cols_names] = df_neigh[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode non ordinal features to perform one hot encoding \"\n",
    "\n",
    "df_neigh['codec']   = df_neigh['codec'].replace(format_mapper)\n",
    "df_neigh['o_codec'] = df_neigh['o_codec'].replace(format_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh = pd.get_dummies(df_neigh, prefix_sep='_', drop_first=True)\n",
    "\n",
    "\" Store the neighbors in a list\"\n",
    "\n",
    "data_neigh = df_neigh.values\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh.append(data_neigh[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 2*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_nb_categ = []\n",
    "j = 0 \n",
    "for name in categorical_cols_names :\n",
    "    mat_nb_categ.append(np.size(data_df[name].unique()))\n",
    "    \n",
    "list_neigh_2 = generate_all_neighbors_2(data_test_copy,numerical_cols,categorical_cols,mat_nb_categ,nb_neighbors)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors_2 = list_neigh_2[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors_2 = np.concatenate((all_neighbors_2, list_neigh_2[i]), axis=0)\n",
    "    \n",
    "df_neigh_2 = pd.DataFrame(data = all_neighbors_2,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh_2[categorical_cols_names] = df_neigh_2[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode non ordinal features to perform one hot encoding \"\n",
    "\n",
    "df_neigh_2['codec']   = df_neigh_2['codec'].replace(format_mapper)\n",
    "df_neigh_2['o_codec'] = df_neigh_2['o_codec'].replace(format_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh_2 = pd.get_dummies(df_neigh_2, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_neigh_2 = df_neigh_2.values\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh_2 = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh_2.append(data_neigh_2[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 3*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youcef\\Desktop\\PFE\\SplitSD4X\\Regression\\SplitSD4X\\supplementary\\neighbors_generation_3.py:14: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  return np.random.multivariate_normal(inst_num,varn,n)\n"
     ]
    }
   ],
   "source": [
    "list_neigh_3 = generate_all_neighbors_3(data_test_copy,numerical_cols,categorical_cols,mat_nb_categ,nb_neighbors)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors_3 = list_neigh_3[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors_3 = np.concatenate((all_neighbors_3, list_neigh_3[i]), axis=0)\n",
    "    \n",
    "df_neigh_3 = pd.DataFrame(data = all_neighbors_3,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh_3[categorical_cols_names] = df_neigh_3[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "df_neigh_3['codec']   = df_neigh_3['codec'].replace(format_mapper)\n",
    "df_neigh_3['o_codec'] = df_neigh_3['o_codec'].replace(format_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh_3 = pd.get_dummies(df_neigh_3, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_neigh_3 = df_neigh_3.values\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh_3 = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh_3.append(data_neigh_3[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation (*Version 4*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = []\n",
    "list_neigh_4 = generate_all_neighbors_4(data_test_copy,numerical_cols,categorical_cols,mat_nb_categ,nb_neighbors,special)\n",
    "\n",
    "\" store all the neighbors together \"\n",
    "n = np.size(data_test_copy,0)\n",
    "all_neighbors_4 = list_neigh_4[0]\n",
    "for i in range(1,n) :\n",
    "    all_neighbors_4 = np.concatenate((all_neighbors_4, list_neigh_4[i]), axis=0)\n",
    "    \n",
    "all_neighbors_4[:,5] = all_neighbors_4[:,5] + 1 \n",
    "all_neighbors_4[:,7] = all_neighbors_4[:,7] + 1 \n",
    "all_neighbors_4[:,11] = all_neighbors_4[:,11] + 1 \n",
    "\n",
    "df_neigh_4 = pd.DataFrame(data = all_neighbors_4,columns= numerical_cols_names + categorical_cols_names)\n",
    "df_neigh_4[categorical_cols_names] = df_neigh_4[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "\" Decode all the data neighbors to perform one hot encoding \"\n",
    "df_neigh_4['codec']   = df_neigh_4['codec'].replace(format_mapper)\n",
    "df_neigh_4['o_codec'] = df_neigh_4['o_codec'].replace(format_mapper)\n",
    "\n",
    "\" One hot encoding \"\n",
    "df_neigh_4 = pd.get_dummies(df_neigh_4, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_neigh_4 = df_neigh_4.values\n",
    "n = np.size(data_test_copy,0)\n",
    "list_neigh_4 = []\n",
    "j = 0\n",
    "for i in range(0,n):\n",
    "    list_neigh_4.append(data_neigh_4[j:(j+nb_neighbors),:])\n",
    "    j += nb_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One hot encoding for the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df = pd.get_dummies(data_train_df, prefix_sep='_', drop_first=True)\n",
    "\n",
    "data_train = data_train_df.values\n",
    "target_train = target_train_df.values\n",
    "\n",
    "data_test_df = pd.get_dummies(data_test_df, prefix_sep='_', drop_first=True)\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Sklearn MLP regressor \"\n",
    "\n",
    "mlp = make_pipeline(StandardScaler(),\n",
    "                    MLPRegressor(hidden_layer_sizes=(50, 50),\n",
    "                                 tol=1e-2, \n",
    "                                 max_iter=1000, \n",
    "                                 random_state=0))\n",
    "model_nt = mlp.fit(data_train, target_train)\n",
    "target_pred_nt = model_nt.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of Split Based Selection Form Algorithm : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(numerical_cols)\n",
    "nb_models = 100\n",
    "(L_Subgroups_1,P_1) = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh,split_point)\n",
    "(L_Subgroups_2,P_2) = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_2,split_point)\n",
    "(L_Subgroups_3,P_3) = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_3,split_point)\n",
    "(L_Subgroups_4,P_4) = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_4,split_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Define the functions to save and load data \"\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE DATA'\n",
    "\n",
    "path = './saved_data/'\n",
    "save_obj(data_train, path + 'data_train')\n",
    "save_obj(target_train, path + 'target_train')\n",
    "save_obj(data_test, path  + 'data_test')\n",
    "save_obj(target_test, path + 'target_test')\n",
    "save_obj(list_neigh, path   + 'list_neighbors_1')\n",
    "save_obj(list_neigh_2, path + 'list_neighbors_2')\n",
    "save_obj(list_neigh_3, path + 'list_neighbors_3')\n",
    "save_obj(list_neigh_4, path + 'list_neighbors_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE LIST OF THE SUBGROUPS'\n",
    "save_obj(L_Subgroups_1, path + 'list_subgroups_1')\n",
    "save_obj(L_Subgroups_2, path + 'list_subgroups_2')\n",
    "save_obj(L_Subgroups_3, path + 'list_subgroups_3')\n",
    "save_obj(L_Subgroups_4, path + 'list_subgroups_4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
