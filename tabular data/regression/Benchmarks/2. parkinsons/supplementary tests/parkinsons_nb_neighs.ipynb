{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the libraries \" \n",
    "\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the scripts of SD for Explaining and the supplementary scripts for neighbors generation\"\n",
    "\n",
    "absFilePath = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "newPath = os.path.join(absFilePath, 'SplitSD4X\\\\')\n",
    "sys.path.append(newPath)\n",
    "\n",
    "newPath_supp = os.path.join(newPath, 'supplementary')\n",
    "sys.path.append(newPath_supp)\n",
    "\n",
    "from fill_missing_values import *\n",
    "from missing_values_table import *\n",
    "from subgroups_discovery import *\n",
    "\n",
    "from neighbors_generation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Loading the dataset \"\n",
    "datasets_path = os.path.join(absFilePath, 'Datasets\\\\')\n",
    "url = datasets_path + 'data_parkinsons_updrs.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop(['subject#','motor_UPDRS'],axis =1)\n",
    "\n",
    "\" Decode Categorical Features \"\n",
    "\n",
    "sex_mapper = {0 : 'male', \n",
    "              1 : 'female' }\n",
    "sex_mapper_inv = dict(map(reversed, sex_mapper.items()))\n",
    "df['sex'] = df['sex'].replace(sex_mapper)\n",
    "\n",
    "\" separate the data and the target \"\n",
    "data_df = df.drop(columns=['total_UPDRS'])\n",
    "target_df = df['total_UPDRS']\n",
    "\n",
    "\" calculate the categorical features mask \"\n",
    "categorical_feature_mask = (data_df.dtypes == object)\n",
    "categorical_cols_names = data_df.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols_names = data_df.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "\" if no values missed we execute this code : \"\n",
    "data_df = pd.concat([data_df[numerical_cols_names], data_df[categorical_cols_names]],axis = 1)\n",
    "\n",
    "\" Encoding categorical features \"\n",
    "data_df['sex'] = data_df['sex'].replace(sex_mapper_inv)\n",
    "\n",
    "data_target_df = pd.concat([data_df, target_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" generate the Test SET \"\n",
    "nb_test_instances = 1000 \n",
    "test_df = data_target_df.sample(n=nb_test_instances)\n",
    "data_test_df = test_df.drop(columns=['total_UPDRS'])\n",
    "target_test_df = test_df['total_UPDRS']\n",
    "\n",
    "\" generate the Training SET \"\n",
    "train_df = pd.concat([data_target_df,test_df]).drop_duplicates(keep=False)\n",
    "data_train_df = train_df.drop(columns=['total_UPDRS'])\n",
    "target_train_df = train_df['total_UPDRS']\n",
    "\n",
    "\" Extract values of the test set to generate the neighbors\"\n",
    "\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values\n",
    "\n",
    "numerical_cols = np.arange(0,len(numerical_cols_names)) \n",
    "categorical_cols = np.arange(len(numerical_cols_names),data_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nb_neighbors = [20,50,100]\n",
    "l_list_neigh = []\n",
    "\n",
    "for nb_neighbors in l_nb_neighbors :\n",
    "    \n",
    "    list_neigh = generate_all_neighbors(data_test,numerical_cols,categorical_cols,nb_neighbors)\n",
    "\n",
    "    \" store all the neighbors together \"\n",
    "    n = np.size(data_test,0)\n",
    "    all_neighbors = list_neigh[0]\n",
    "    for i in range(1,n) :\n",
    "        all_neighbors = np.concatenate((all_neighbors, list_neigh[i]), axis=0)\n",
    "\n",
    "    df_neigh = pd.DataFrame(data = all_neighbors,columns= numerical_cols_names + categorical_cols_names)\n",
    "    df_neigh[categorical_cols_names] = df_neigh[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "    \" Decode all the data neighbors to perform one hot encoding \"\n",
    "    df_neigh['sex'] = df_neigh['sex'].replace(sex_mapper)\n",
    "\n",
    "    \" One hot encoding \"\n",
    "    df_neigh = pd.get_dummies(df_neigh, prefix_sep='_', drop_first=True)\n",
    "\n",
    "    \" Store the neighbors in a list\"\n",
    "\n",
    "    data_neigh = df_neigh.values\n",
    "    n = np.size(data_test,0)\n",
    "    list_neigh = []\n",
    "    j = 0\n",
    "    for i in range(0,n):\n",
    "        list_neigh.append(data_neigh[j:(j+nb_neighbors),:])\n",
    "        j += nb_neighbors\n",
    "    \n",
    "    l_list_neigh.append(list_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neigh_20  = l_list_neigh[0] \n",
    "list_neigh_50  = l_list_neigh[1]\n",
    "list_neigh_100 = l_list_neigh[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One hot encoding for the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df['sex'] = data_train_df['sex'].replace(sex_mapper)\n",
    "\n",
    "data_train_df = pd.get_dummies(data_train_df, prefix_sep='_', drop_first=True)\n",
    "data_train = data_train_df.values\n",
    "target_train = target_train_df.values\n",
    "\n",
    "data_test_df['sex'] = data_test_df['sex'].replace(sex_mapper)\n",
    "\n",
    "data_test_df = pd.get_dummies(data_test_df, prefix_sep='_', drop_first=True)\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Sklearn MLP regressor \"\n",
    "\n",
    "mlp = make_pipeline(StandardScaler(),\n",
    "                    MLPRegressor(hidden_layer_sizes=(50, 50),\n",
    "                                 tol=1e-2, \n",
    "                                 max_iter=1000, \n",
    "                                 random_state=0))\n",
    "model_nt = mlp.fit(data_train, target_train)\n",
    "target_pred_nt = model_nt.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of Split Based Selection Form Algorithm : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(numerical_cols)\n",
    "nb_models = 100\n",
    "(L_Subgroups_20,P_1)  = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_20,split_point)\n",
    "(L_Subgroups_50,P_2)  = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_50,split_point)\n",
    "(L_Subgroups_100,P_3) = SplitBasedSelectionForm(data_test, target_test, nb_models, model_nt, list_neigh_100,split_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Define the functions to save and load data \"\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE DATA'\n",
    "\n",
    "path = './saved_data/'\n",
    "save_obj(data_train, path + 'data_train_n')\n",
    "save_obj(target_train, path + 'target_train_n')\n",
    "save_obj(data_test, path  + 'data_test_n')\n",
    "save_obj(target_test, path + 'target_test_n')\n",
    "save_obj(list_neigh_20, path + 'list_neighbors_20')\n",
    "save_obj(list_neigh_50, path + 'list_neighbors_50')\n",
    "save_obj(list_neigh_100, path + 'list_neighbors_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE LIST OF THE SUBGROUPS'\n",
    "save_obj(L_Subgroups_20, path  + 'list_subgroups_20')\n",
    "save_obj(L_Subgroups_50, path  + 'list_subgroups_50')\n",
    "save_obj(L_Subgroups_100, path + 'list_subgroups_100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
