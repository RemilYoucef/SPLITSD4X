{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the libraries \" \n",
    "\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Import the scripts of SD for Explaining and the supplementary scripts for neighbors generation\"\n",
    "\n",
    "absFilePath = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "newPath = os.path.join(absFilePath, 'SplitSD4X\\\\')\n",
    "sys.path.append(newPath)\n",
    "\n",
    "newPath_supp = os.path.join(newPath, 'supplementary')\n",
    "sys.path.append(newPath_supp)\n",
    "\n",
    "from fill_missing_values import *\n",
    "from missing_values_table import *\n",
    "from subgroups_discovery import *\n",
    "\n",
    "from neighbors_generation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Loading the dataset \"\n",
    "datasets_path = os.path.join(absFilePath, 'Datasets\\\\')\n",
    "url = datasets_path + 'data_bike_hour.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop(['instant','dteday','casual','registered'],axis =1)\n",
    "\n",
    "\" Handling some data \"\n",
    "df = df.drop(df[df.weathersit == 4].index)\n",
    "df[df[\"weathersit\"] == 4]\n",
    "\n",
    "\" Decode Categorical Features \"\n",
    "\n",
    "weekday_mapper = {0 : 'Sun', \n",
    "                  1 : 'Mon',\n",
    "                  2 : 'Tue',\n",
    "                  3 : 'Wed',\n",
    "                  4 : 'Thu',\n",
    "                  5 : 'Fri',\n",
    "                  6 : 'Sat' }\n",
    "weekday_mapper_inv = dict(map(reversed, weekday_mapper.items()))\n",
    "df['weekday'] = df['weekday'].replace(weekday_mapper)\n",
    "\n",
    "\n",
    "holiday_mapper = {0 : 'No_Holiday',\n",
    "                  1 : 'Holiday'}\n",
    "holiday_mapper_inv = dict(map(reversed, holiday_mapper.items()))\n",
    "df['holiday'] = df['holiday'].replace(holiday_mapper)\n",
    "\n",
    "\n",
    "\n",
    "workingday_mapper = {0 : 'No_Working_Day',\n",
    "                     1 : 'Working_Day'}\n",
    "workingday_mapper_inv = dict(map(reversed, workingday_mapper.items()))\n",
    "df['workingday'] = df['workingday'].replace(workingday_mapper)\n",
    "\n",
    "\n",
    "\n",
    "season_mapper = {1 : 'Spring',\n",
    "                 2 : 'Summer',\n",
    "                 3 : 'Fall',\n",
    "                 4 : 'Winter'}\n",
    "season_mapper_inv = dict(map(reversed, season_mapper.items()))\n",
    "df['season'] = df['season'].replace(season_mapper)\n",
    "\n",
    "wethersit_mapper = {1 : 'Good',\n",
    "                    2 : 'Misty',\n",
    "                    3 : 'Rain_Snow_Storm'}\n",
    "wethersit_mapper_inv = dict(map(reversed, wethersit_mapper.items()))\n",
    "df['weathersit'] = df['weathersit'].replace(wethersit_mapper)\n",
    "\n",
    "\n",
    "\n",
    "mnth_mapper = {1  : 'Jan',\n",
    "               2  : 'Feb',\n",
    "               3  : 'Mar',\n",
    "               4  : 'Apr',\n",
    "               5  : 'May',\n",
    "               6  : 'Jun',\n",
    "               7  : 'Jul',\n",
    "               8  : 'Aug',\n",
    "               9  : 'Sep',\n",
    "               10 : 'Oct',\n",
    "               11 : 'Nov',\n",
    "               12 : 'Dec'}\n",
    "mnth_mapper_inv = dict(map(reversed, mnth_mapper.items()))\n",
    "df['mnth'] = df['mnth'].replace(mnth_mapper)\n",
    "\n",
    "\n",
    "yr_mapper = {0 : '2011',\n",
    "             1 : '2012'}\n",
    "yr_mapper_inv = dict(map(reversed, yr_mapper.items()))\n",
    "df['yr'] = df['yr'].replace(yr_mapper)\n",
    "\n",
    "# Numerical Features\n",
    "df['temp'] = df['temp'] * (39 - (-8)) + (-8)\n",
    "df['atemp'] = df['atemp'] * (50 - (16)) + (16)\n",
    "df['windspeed'] = df['windspeed'] * 67\n",
    "df['hum'] = df['hum']*100\n",
    "\n",
    "\" separate the data and the target \"\n",
    "data_df = df.drop(columns=['cnt'])\n",
    "target_df = df['cnt']\n",
    "\n",
    "\" calculate the categorical features mask \"\n",
    "categorical_feature_mask = (data_df.dtypes == object)\n",
    "categorical_cols_names = data_df.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols_names = data_df.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "\" if no values missed we execute this code : \"\n",
    "data_df = pd.concat([data_df[numerical_cols_names], data_df[categorical_cols_names]],axis = 1)\n",
    "\n",
    "\" Encoding categorical features\"\n",
    "\n",
    "data_df['weekday'] = data_df['weekday'].replace(weekday_mapper_inv)\n",
    "data_df['holiday'] = data_df['holiday'].replace(holiday_mapper_inv)\n",
    "data_df['workingday'] = data_df['workingday'].replace(workingday_mapper_inv)\n",
    "data_df['season'] = data_df['season'].replace(season_mapper_inv)\n",
    "data_df['weathersit'] = data_df['weathersit'].replace(wethersit_mapper_inv)\n",
    "data_df['mnth'] = data_df['mnth'].replace(mnth_mapper_inv)\n",
    "data_df['yr'] = data_df['yr'].replace(yr_mapper_inv)\n",
    "\n",
    "data_target_df = pd.concat([data_df, target_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" generate the Test SET \"\n",
    "nb_test_instances = 1000 \n",
    "test_df = data_target_df.sample(n=nb_test_instances)\n",
    "data_test_df = test_df.drop(columns=['cnt'])\n",
    "target_test_df = test_df['cnt']\n",
    "\n",
    "\" generate the Training SET \"\n",
    "train_df = pd.concat([data_target_df,test_df]).drop_duplicates(keep=False)\n",
    "data_train_df = train_df.drop(columns=['cnt'])\n",
    "target_train_df = train_df['cnt']\n",
    "\n",
    "\" Extract values of the test set to generate the neighbors\"\n",
    "\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values\n",
    "\n",
    "numerical_cols = np.arange(0,len(numerical_cols_names)) \n",
    "categorical_cols = np.arange(len(numerical_cols_names),data_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nb_neighbors = [20,50,100]\n",
    "l_list_neigh = []\n",
    "\n",
    "for nb_neighbors in l_nb_neighbors : \n",
    "    \n",
    "    list_neigh = generate_all_neighbors(data_test,numerical_cols,categorical_cols,nb_neighbors)\n",
    "\n",
    "    \" store all the neighbors together \"\n",
    "    n = np.size(data_test,0)\n",
    "    all_neighbors = list_neigh[0]\n",
    "    for i in range(1,n) :\n",
    "        all_neighbors = np.concatenate((all_neighbors, list_neigh[i]), axis=0)\n",
    "\n",
    "    \" One hot encoding \"\n",
    "\n",
    "    df_neigh = pd.DataFrame(data = all_neighbors,columns= numerical_cols_names + categorical_cols_names)\n",
    "    df_neigh[categorical_cols_names] = df_neigh[categorical_cols_names].astype(int,errors='ignore')\n",
    "\n",
    "    \" Decode all the data neighbors to perform one hot encoding \"\n",
    "    df_neigh['weekday'] = df_neigh['weekday'].replace(weekday_mapper)\n",
    "    df_neigh['holiday'] = df_neigh['holiday'].replace(holiday_mapper)\n",
    "    df_neigh['workingday'] = df_neigh['workingday'].replace(workingday_mapper)\n",
    "    df_neigh['season'] = df_neigh['season'].replace(season_mapper)\n",
    "    df_neigh['weathersit'] = df_neigh['weathersit'].replace(wethersit_mapper)\n",
    "    df_neigh['mnth'] = df_neigh['mnth'].replace(mnth_mapper)\n",
    "    df_neigh['yr'] = df_neigh['yr'].replace(yr_mapper)\n",
    "\n",
    "    \" One hot encoding \"\n",
    "    df_neigh = pd.get_dummies(df_neigh, prefix_sep='_', drop_first=True)\n",
    "\n",
    "    \" Store the neighbors in a list\"\n",
    "\n",
    "    data_neigh = df_neigh.values\n",
    "    n = np.size(data_test,0)\n",
    "    list_neigh = []\n",
    "    j = 0\n",
    "    for i in range(0,n):\n",
    "        list_neigh.append(data_neigh[j:(j+nb_neighbors),:])\n",
    "        j += nb_neighbors\n",
    "    \n",
    "    l_list_neigh.append(list_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neigh_20  = l_list_neigh[0] \n",
    "list_neigh_50  = l_list_neigh[1]\n",
    "list_neigh_100 = l_list_neigh[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One hot encoding for the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df['weekday'] = data_train_df['weekday'].replace(weekday_mapper)\n",
    "data_train_df['holiday'] = data_train_df['holiday'].replace(holiday_mapper)\n",
    "data_train_df['workingday'] = data_train_df['workingday'].replace(workingday_mapper)\n",
    "data_train_df['season'] = data_train_df['season'].replace(season_mapper)\n",
    "data_train_df['weathersit'] = data_train_df['weathersit'].replace(wethersit_mapper)\n",
    "data_train_df['mnth'] = data_train_df['mnth'].replace(mnth_mapper)\n",
    "data_train_df['yr'] = data_train_df['yr'].replace(yr_mapper)\n",
    "\n",
    "data_train_df = pd.get_dummies(data_train_df, prefix_sep='_', drop_first=True)\n",
    "data_train = data_train_df.values\n",
    "target_train = target_train_df.values\n",
    "\n",
    "data_test_df['weekday'] = data_test_df['weekday'].replace(weekday_mapper)\n",
    "data_test_df['holiday'] = data_test_df['holiday'].replace(holiday_mapper)\n",
    "data_test_df['workingday'] = data_test_df['workingday'].replace(workingday_mapper)\n",
    "data_test_df['season'] = data_test_df['season'].replace(season_mapper)\n",
    "data_test_df['weathersit'] = data_test_df['weathersit'].replace(wethersit_mapper)\n",
    "data_test_df['mnth'] = data_test_df['mnth'].replace(mnth_mapper)\n",
    "data_test_df['yr'] = data_test_df['yr'].replace(yr_mapper)\n",
    "\n",
    "data_test_df = pd.get_dummies(data_test_df, prefix_sep='_', drop_first=True)\n",
    "data_test = data_test_df.values\n",
    "target_test = target_test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Sklearn MLP regressor \"\n",
    "\n",
    "mlp = make_pipeline(StandardScaler(),\n",
    "                    MLPRegressor(hidden_layer_sizes=(50, 50),\n",
    "                                 tol=1e-2, \n",
    "                                 max_iter=1000, \n",
    "                                 random_state=0))\n",
    "model_nt = mlp.fit(data_train, target_train)\n",
    "target_pred_nt = model_nt.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of Split Based Selection Form Algorithm : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = len(numerical_cols)\n",
    "nb_models = 100\n",
    "(L_Subgroups_20,P_1)  = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_20,split_point)\n",
    "(L_Subgroups_50,P_2)  = SplitBasedSelectionForm (data_test, target_test, nb_models, model_nt, list_neigh_50,split_point)\n",
    "(L_Subgroups_100,P_3) = SplitBasedSelectionForm(data_test, target_test, nb_models, model_nt, list_neigh_100,split_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Define the functions to save and load data \"\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE DATA'\n",
    "\n",
    "path = './saved_data/'\n",
    "save_obj(data_train, path + 'data_train_n')\n",
    "save_obj(target_train, path + 'target_train_n')\n",
    "save_obj(data_test, path  + 'data_test_n')\n",
    "save_obj(target_test, path + 'target_test_n')\n",
    "save_obj(list_neigh_20, path + 'list_neighbors_20')\n",
    "save_obj(list_neigh_50, path + 'list_neighbors_50')\n",
    "save_obj(list_neigh_100, path + 'list_neighbors_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SAVE THE LIST OF THE SUBGROUPS'\n",
    "save_obj(L_Subgroups_20, path  + 'list_subgroups_20')\n",
    "save_obj(L_Subgroups_50, path  + 'list_subgroups_50')\n",
    "save_obj(L_Subgroups_100, path + 'list_subgroups_100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
